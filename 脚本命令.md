# 环境准备

安装驱动


``` shell
# 进入docker
cd /data/infer/
./docker_run_bmnnsdk.sh
cd bmnnsdk-bm1684_vSA5
source scripts/envsetup_pcie.sh
pip3 install bmnet/bmneto/bmneto*.whl -f /workspace/wheels --no-index
pip3 install pycocotools -f /workspace/wheels --no-index
pip3 install tensorflow_cpu==1.15.0 -f /workspace/wheels --no-index
pip3 install easydict -f /workspace/wheels --no-index
export PYTHONPATH=/workspace/BMService/python:$PYTHONPATH
```
# inception_v3

## 获取代码与生成离线模型

### 准备代码

代码链接 `https://github.com/tensorflow/models/tree/v1.13.0/research/slim`

``` shell
  cd ~/workspace/code
  git clone https://github.com/tensorflow/models.git
  cd models/research/slim
  git checkout v1.13.0
  wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz
  tar xvf inception_v3_2016_08_28.tar.gz
```

### 生成离线模型

  1. 准备tf1.x下环境（tensorflow1.15或1.14均可）
  2. cp /workspace/doc/export_inception_v3.py /workspace/code/models/research/slim
  3. 执行命令 ``python3 export_inception_v3.py``, 会在当前生成`inception_v3.pb`离线模型
  4. ``cp inception_v3.pb /workspace/compile_model``

## 生成运行模型(bmodel)

### 生成fp32模型并运行

``` shell
cd /workspace/compile_model
python3 -m bmnett --model=inception_v3.pb \
                  --input_names "input" \
                  --shapes "[1,299,299,3]" \
                  --output_names "InceptionV3/Predictions/Reshape_1" \
                  --outdir inception_fp32 \
                  --enable_profile=1 \
                  --target=BM1684 \
                  --opt=2 \
                  --cmp=False \
                  --v=4 2>&1 | tee inception_fp32.log
cp inception_fp32/compilation.bmodel /workspace/BMService/models/inception/fp32.bmodel

cd /workspace/BMService/build/
./BMService-inception /workspace/dataset/ILSVRC2012_val/ \
                      /workspace/BMService/models/inception/fp32.bmodel \
                      2>&1 | tee inception_fp32_run.log
```

### 生成fix8模型

  1. 生成umodel
  ``` shell
cd /workspace/compile_model
python3 -m bmnett --model=inception_v3.pb \
                  --mode GenUmodel \
                  --input_names "input" \
                  --shapes "[4,299,299,3]" \
                  --output_names "InceptionV3/Predictions/Reshape_1" \
                  --enable_profile=1 \
                  --target=BM1684 \
                  --outdir inception_umodel \
                  --v=4 2>&1 | tee inception_umodel.log
  ```

  2. 准备数据集
``` shell
cp -r /workspace/BMService/data/ILSVRC2012/images /workspace/compile_model/dataset
pushd /workspace/compile_model/dataset
ls *.JPEG >imagelist.txt
popd
convert_imageset /workspace/compile_model/dataset/ \
                 /workspace/compile_model/dataset/imagelist.txt \
                 /workspace/compile_model/image_lmdb
```

  3. 量化模型
  ``` shell
  # 生成量化模板文本文件
python3 -m ufw.tools.to_umodel \
      -u inception_umodel/bm_network_bmnett.fp32umodel  \
      -p INCEPTION \
      -D /workspace/compile_model/image_lmdb

  # 开始量化
calibration_use_pb quantize \
      -model inception_umodel/bm_network_bmnett_test_fp32.prototxt \
      -weights inception_umodel/bm_network_bmnett.fp32umodel \
      -iterations=20 \
      --debug_log_level=2 \
      --fpfwd32="" \
      -bitwidth=TO_INT8  2>&1 | tee inception_cali.log
  ```


  4. 生成最终fix8b模型并运行
  ``` shell
bmnetu --weight inception_umodel/bm_network_bmnett.int8umodel \
       --model inception_umodel/bm_network_bmnett_deploy_int8_unique_top.prototxt \
       --outdir inception_fix8b \
       --max_n 4 \
       --opt=2 \
       --cmp=0 \
       --v=4 2>&1 | tee inception_fix8b.log
cp inception_fix8b/compilation.bmodel /workspace/BMService/models/inception/fix8b_b4.bmodel

cd /workspace/BMService/build/
./BMService-inception /workspace/dataset/ILSVRC2012_val/ \
                      /workspace/BMService/models/inception/fix8b_b4.bmodel \
                      2>&1 | tee inception_fix8b_run.log
  ```

# yolov3

## 准备代码与生成离线模型
``` shell
cd /workspace/code
git clone https://github.com/YunYang1994/tensorflow-yolov3.git
cd tensorflow-yolov3
pip3 install -r ./docs/requirements.txt
cd checkpoint
wget https://github.com/YunYang1994/tensorflow-yolov3/releases/download/v1.0/yolov3_coco.tar.gz
tar xvf yolov3_coco.tar.gz
cd ..
python3 convert_weight.py
python3 freeze_graph.py
cp yolov3_coco.pb ~/workspace/compile_model/
```

## 生成fp32模型并运行 
``` shell
cd /workspace/compile_model
python3 -m bmnett --model "yolov3_coco.pb" \
                  --input_names "input/input_data" \
                  --shapes "[1,544,544,3]" \
                  --target BM1684 \
                  --opt 2 \
                  --cmp False \
                  --outdir yolov3_fp32\
                  --v 4 2>&1 | tee yolov3_fp32.log
cp yolov3_fp32/compilation.bmodel /workspace/BMService/models/yolov3/fp32.bmodel
cd /workspace/BMService/build
./BMService-yolov3 /workspace/dataset/val2017/ \
                   /workspace/BMService/models/yolov3/fp32.bmodel \
                   2>&1 | tee yolov3_fp32_run.log
mv yolov3_result.json yolov3_fp32_result.json
python3 ../tools/coco_eval.py yolov3_fp32_result.json ../data/coco/instances_val2017.json 2>&1 | tee yolov3_fp32_acc.log
```

## 生成fix8b模型并运行

  1. 生成umodel
``` shell
cd /workspace/compile_model
python3 -m bmnett --model "yolov3_coco.pb" \
                  --mode GenUmodel \
                  --input_names "input/input_data" \
                  --shapes "[1,544,544,3]" \
                  --target BM1684 \
                  --cmp False \
                  --outdir yolov3_umodel \
                  --v 4 2>&1 | tee yolov3_umodel.log
```

  2. 制作数据集
``` shell
cp -r /workspace/BMService/data/coco/images /workspace/compile_model/coco_dataset
pushd /workspace/compile_model/coco_dataset
ls *.jpg >imagelist.txt
popd
convert_imageset /coco_dataset/ \
                 /coco_dataset/imagelist.txt \
                 /coco_lmdb
```

  3. 生成量化文本文件模板
``` shell
python3 -m ufw.tools.to_umodel -u yolov3_umodel/bm_network_bmnett.fp32umodel \
                               -D /workspace/compile_model/coco_lmdb
```

  4. 修改模板(yolov3_umodel/bm_network_bmnett_test_fp32.prototxt)
  * 去掉首层的transpose
  * 数据层加上预处理变换参数, 要与代码中前处理完全对齐

  ``` shell
  vi yolov3_umodel/bm_network_bmnett_test_fp32.prototxt
  transform_param {
    transform_op {
      op: STAND
      scale: 0.003921568859368563
    }
    transform_op {
      op: RESIZE
      resize_h: 544
      resize_w: 544
      bgr2rgb: true
    }
  }
  ```

  5. 量化模型
  ``` shell
calibration_use_pb quantize \
    -model yolov3_umodel/bm_network_bmnett_test_fp32.prototxt \
    -weights yolov3_umodel/bm_network_bmnett.fp32umodel \
    -iterations=20 \
    --debug_log_level=2 \
    -bitwidth=TO_INT8 \
    --fpfwd_outputs="conv_sbbox/BiasAdd@otrans,conv_mbbox/BiasAdd@otrans,conv_lbbox/BiasAdd@otrans" \
    2>&1 | tee yolov3_cali.log
  ```

  6. fix8b模型并运行
  ``` shell
bmnetu \
  --model=yolov3_umodel/bm_network_bmnett_deploy_int8_unique_top.prototxt \
  --weight=yolov3_umodel/bm_network_bmnett.int8umodel \
  --opt=2 \
  --max_n=4 \
  --enable_profile=1 \
  --outdir=yolov3_fix8b \
  --cmp=0 \
  --v=4 2>&1 | tee yolov3_fix8b.log
cp yolov3_fix8b/compilation.bmodel /workspace/BMService/models/yolov3/fix8b_b4.bmodel
cd /workspace/BMService/build
./BMService-yolov3 /workspace/dataset/val2017/ \
                   /workspace/BMService/models/yolov3/fix8b_b4.bmodel \
                   2>&1 | tee yolov3_fix8b_b4_run.log
mv yolov3_result.json yolov3_fix8b_result.json
python3 ../tools/coco_eval.py yolov3_fix8b_result.json ../data/coco/instances_val2017.json 2>&1 | tee yolov3_fix8b_acc.log

  ```

# ssd_resnet34:

## 离线模型

``` shell
cd ~/workspace/compile_model
wget https://zenodo.org/record/3228411/files/resnet34-ssd1200.onnx
```

## 生成fp32模型并运行

``` shell
cd /workspace/compile_model
python3 -m bmneto --model resnet34-ssd1200.onnx \
                  --input_names "image"  \
                  --shapes "[1,3,1200,1200]" \
                  --output_names "Concat_470,Concat_471" \
                  --target BM1684 \
                  --enable_profile True \
                  --outdir ssd_resnet34_fp32\
                  --opt=2 \
                  --cmp False  \
                  --v=4 2>&1 | tee ssd_resnet34_fp32.log
cp ssd_resnet34_fp32/compilation.bmodel /workspace/BMService/models/ssd_resnet34/fp32.bmodel
cd /workspace/BMService/build
./BMService-ssd_resnet34 /workspace/dataset/val2017/ \
                   /workspace/BMService/models/ssd_resnet34/fp32.bmodel \
                   2>&1 | tee ssd_resnet34_fp32_run.log
mv ssd_resnet34_result.json ssd_resnet34_fp32_result.json
python3 ../tools/coco_eval.py ssd_resnet34_fp32_result.json ../data/coco/instances_val2017.json 2>&1 | tee ssd_resnet34_fp32_acc.log
```

## 生成fix8b模型并运行

  1. 生成umodel
``` shell
cd /workspace/compile_model
python3 -m bmneto --model resnet34-ssd1200.onnx \
                  --input_names "image"  \
                  --shapes "[1,3,1200,1200]" \
                  --output_names "Concat_470,Concat_471" \
                  --target BM1684 \
                  --enable_profile True \
                  --mode GenUmodel \
                  --outdir ssd_umodel \
                  --cmp False  \
                  --v=4 2>&1 | tee o.log
```

  2. 制作数据集(同yolov3, 可略过)
``` shell
cp -r /workspace/BMService/data/coco/images /workspace/compile_model/coco_dataset
pushd /workspace/compile_model/coco_dataset
ls *.jpg >imagelist.txt
popd
convert_imageset /workspace/compile_model/coco_dataset/ \
                 /workspace/compile_model/coco_dataset/imagelist.txt \
                 /workspace/compile_model/coco_lmdb
```

  3. 生成量化文本文件模板
``` shell
python3 -m ufw.tools.to_umodel \
        -u ssd_umodel/resnet34-ssd1200.onnx_bmnetm.fp32umodel \
        -D /workspace/compile_model/coco_lmdb
```

  4. 修改模板

    ssd_umodel/resnet34-ssd1200.onnx_bmnetm_test_fp32.prototxt
  * 数据层加上预处理变换参数, 要与代码中前处理完全对齐

  ``` shell
  transform_param {
    transform_op {
      op: STAND
      scale: 0.003921568859368563
    }
    transform_op {
      op: RESIZE
      resize_h: 1200
      resize_w: 1200 
      bgr2rgb: true
    }
  }
  ```

  5. 量化模型
  ``` shell
calibration_use_pb quantize \
    -model ssd_umodel/resnet34-ssd1200.onnx_bmnetm_test_fp32.prototxt \
    -weights ssd_umodel/resnet34-ssd1200.onnx_bmnetm.fp32umodel \
    -iterations=20 \
        --fpfwd_outputs="Conv_349,Conv_338,Conv_360,Conv_371,Conv_393,Conv_382,Conv_437,Conv_448,Conv_459,Conv_426,Conv_415,Conv_404" \
    --debug_log_level=2 \
    -bitwidth=TO_INT8 \
    2>&1 | tee ssd_resnet34_cali.log
  ```


  6. fix8b模型并运行
  ``` shell
cd /workspace/compile_model
bmnetu \
    --model ssd_umodel/resnet34-ssd1200.onnx_bmnetm_deploy_int8_unique_top.prototxt \
    --weight ssd_umodel/resnet34-ssd1200.onnx_bmnetm.int8umodel \
    --max_n 4 \
    --enable_profile=1 \
    --outdir ssd_resnet34_fix8b \
    --opt=2 \
    --cmp=0 \
    --v=4 2>&1 | tee ssd_resnet34_fix8b.log
cp ssd_resnet34_fix8b/compilation.bmodel /workspace/BMService/models/ssd_resnet34/fix8b_b4.bmodel
cd /workspace/BMService/build
./BMService-ssd_resnet34 /workspace/dataset/val2017/ \
                         /workspace/BMService/models/ssd_resnet34/fix8b_b4.bmodel \
                         2>&1 | tee ssd_resnet34_fix8b_b4_run.log
mv ssd_resnet34_result.json ssd_resnet34_fix8b_result.json
python3 ../tools/coco_eval.py ssd_resnet34_fix8b_result.json ../data/coco/instances_val2017.json 2>&1 | tee ssd_resnet34_fix8b_acc.log
  ```


# bert

## 准备离线模型

* 下载 https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip
* 下载 squad数据集及评估代码(dev-v1.1.json, evaluate-v1.1.py)
* 执行如下命令
``` shell
git clone https://github.com/google-research/bert/
cd bert

#将uncased_L-12_H-768_A-12.zip复制到bert目录下并解压
unzip ../uncased_L-12_H-768_A-12.zip

#复制数据集 squad到bert目录下
cp -r ../squad .

# finetune
export BERT_BASE_DIR=uncased_L-12_H-768_A-12
export SQUAD_DIR=squad
python3 run_squad.py \
      --vocab_file=$BERT_BASE_DIR/vocab.txt \
      --bert_config_file=$BERT_BASE_DIR/bert_config.json \
      --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \
      --do_train=True \
      --train_file=$SQUAD_DIR/train-v1.1.json \
      --do_predict=True \
      --predict_file=$SQUAD_DIR/dev-v1.1.json \
      --train_batch_size=8 \
      --learning_rate=3e-5 \
      --num_train_epochs=2.0 \
      --max_seq_length=384 \
      --doc_stride=128 \
      --output_dir=squad_out 2>&1 | tee finetune.log

# 生成离线模型
cd /workspace/code/bert
MAX_BATCH=1
cp /workspace/doc/freeze_squad.py .

# 这里的model.ckpt-21899要根据finetune结果选择，会在当前目录下生成squad_graph.pb
python3 freeze_squad.py \
    --bert_config_file uncased_L-12_H-768_A-12/bert_config.json \
    --init_checkpoint squad_out/model.ckpt-21899 \
    --batch_size $MAX_BATCH \
    --output_name squad_graph.pb
cp squad_graph.pb /workspace/compile_model
```

## 生成fp32模型并运行
``` shell
cd /workspace/compile_model
MAX_BATCH=1
python3 -m bmnett --model squad_graph.pb \
                  --target BM1684 \
                  --shapes="[$MAX_BATCH,384], [$MAX_BATCH,384], [$MAX_BATCH,384]"  \
                  --input_names="input_ids, input_mask, segment_ids" \
                  --descs="[0,int32,0,256],[1, int32, 0, 2], [2, int32, 0, 2]" \
                  --outdir squad_fp32 \
                  --enable_profile=True \
                  --cmp=False \
                  --opt=2 \
                  --v=4 2>&1 | tee squad_fp32.log

cp squad_fp32/compilation.bmodel /workspace/BMService/models/bert_squad/fp32.bmodel

# BMService的python环境准备(如已经准备好，可略过)
pushd /workspace/BMService
mkdir -p python/bmservice/lib
mkdir build && cd build && cmake ../ && make -j && cd ..
cp build/libbmservice.so python/bmservice/lib
export PYTHONPATH=/workspace/BMService/python:$PYTHONPATH
popd


# 准备运行脚本
# 将bmservice_squad.py复制到bert目录下
cd /workspace/code/bert
cp /workspace/BMService/examples/bert/bmservice_squad.py .
# 开始运行
python3 bmservice_squad.py squad/dev-v1.1.json /workspace/BMService/models/bert_squad/fp32.bmodel 2>&1 | tee bert_fp32_run.log
# 评估精度
python3 squad/evaluate-v1.1.py squad/dev-v1.1.json squad_eval_out/predictions.json 2>&1 | bert_fp32_acc.log

```

# 3dunet

## 下载离线模型及代码

``` shell
cd ~/workspace/compile_model
wget https://zenodo.org/record/3928991/files/224_224_160.pb
mv 224_224_160.pb 3dunet_224_224_160.pb

cd ~/workspace/code
git clone https://github.com/mlcommons/inference.git
```

## 生成fp32模型

``` shell
cd /workspace/compile_model
python3 -m bmnett --model 3dunet_224_224_160.pb \
                  --input_names="input" \
                  --shapes="[1,4,224,224,160]" \
                  --output_names="output" \
                  --dyn=false \
                  --outdir=3dunet_fp32 \
                  --enable_profile=false \
                  --cmp=0 \
                  --target=BM1684 \
                  --opt=2 \
                  --v=4 2>&1 | tee 3dunet_fp32.log
cp 3dunet_fp32/compilation.bmodel /workspace/BMService/models/3dunet/fp32.bmodel
```

## 运行(docker外)
``` shell
cd ~/workspace/code/3d-unet-brats19/

# 以下几步需要联网, 而且需要在inference/vision/medical_imaging/3d-unet-bras19
export DOWNLOAD_DATA_DIR=~/workspace/dataset/BraTS/MICCAI_BraTS_2019_Data_Training
# 下载依赖模块及模型
make setup
# 将数据预处理到 build/preprocessed_data
make preprocess_data
mkdir -p build/postprocessed_data/

# 在docker外面，docker里python版本比较老，无法运行
# 复制ex_run.py和bmservice_SUT.py到3d-unet-brats19
cp ~/workspace/BMService/examples/3d-unet-brats19/* .
python3 ex_run.py --backend bmservice \
                  --model ~/workspace/BMService/models/3dunet/fp32.bmodel \
                  --performance_count 100 \
                  --accuracy \
                  2>&1 | tee 3dunet_fp32.log
```

# dlrm

## 下载模型权重文件
git clone https://github.com/facebookresearch/dlrm.git
cd dlrm

## 生成离线模型(docker外)
``` shell
# torch==1.8.0
# pip3 download torch==1.8.0+cpu torchvision==0.9.0+cpu torchaudio==0.8.0 -f ~/workspace/wheels --user --no-index
cd ~/workspace/code/dlrm
cp ~/workspace/doc/export_dlrm.py .
python3 export_dlrm.py 2>&1 | tee dlrm_trace.log
mv dlrm.torchscript.pt /workspace/compile_model

```

## 生成fp32模型并运行

``` shell
cd /workspace/compile_model
DLRM_BATCH=1024
python3 -m bmnetp --target=BM1684 \
--outdir=dlrm_bmodel_b${DLRM_BATCH} \
--model=dlrm.torchscript.pt \
--shapes="[${DLRM_BATCH},13],[26,${DLRM_BATCH}],[${DLRM_BATCH}],[${DLRM_BATCH}],[${DLRM_BATCH}],[${DLRM_BATCH}],[${DLRM_BATCH}],[${DLRM_BATCH}],[${DLRM_BATCH}],[${DLRM_BATCH}],[${DLRM_BATCH}],[${DLRM_BATCH}],[${DLRM_BATCH}],[${DLRM_BATCH}],[${DLRM_BATCH}],[${DLRM_BATCH}],[${DLRM_BATCH}],[${DLRM_BATCH}],[${DLRM_BATCH}],[${DLRM_BATCH}],[${DLRM_BATCH}],[${DLRM_BATCH}],[${DLRM_BATCH}],[${DLRM_BATCH}],[${DLRM_BATCH}],[${DLRM_BATCH}],[${DLRM_BATCH}],[${DLRM_BATCH}]" \
--opt=2 \
--cmp=False \
--enable_profile=true \
--input_structure="0,1,(2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27)" \
--desc "[0,fp32,0,1],[1,int32,0,1][2,int32,0,1],[3,int32,0,1],[4,int32,0,1],[5,int32,0,1],[6,int32,0,1],[7,int32,0,1],[8,int32,0,1],[9,int32,0,1],[10,int32,0,1],[11,int32,0,1],[12,int32,0,1],[13,int32,0,1],[14,int32,0,1],[15,int32,0,1],[16,int32,0,1],[17,int32,0,1],[18,int32,0,1],[19,int32,0,1],[20,int32,0,1],[21,int32,0,1],[22,int32,0,1],[23,int32,0,1],[24,int32,0,1],[25,int32,0,1],[26,int32,0,1],[27,int32,0,1]" \
--v=4 2>&1 | tee dlrm_bmodel_b${DLRM_BATCH}.log

mv dlrm_bmodel_b${DLRM_BATCH}/compilation.bmodel /workspace/BMService/models/dlrm/fp32_b${DLRM_BATCH}.bmodel

cd /workspace/BMService/examples/dlrm
DLRM_BATCH=1024
python3 bmservice_dlrm.py day_23_reordered.npz /workspace/BMService/models/dlrm/fp32_b${DLRM_BATCH}.bmodel 2>&1 | tee dlrm_fp32_b${DLRM_BATCH}_run.log


```
